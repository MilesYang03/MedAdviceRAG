1 Introduction 
Retrieval augmented generation (RAG) is a technique combining large language models with data retrieval in the goal of improving accuracy and reliability of LLM text generation. LLM’s face the common problem of AI hallucinations, which is when generated text becomes inaccurate and unreliable. This project explores the possibility of applying RAG in answering medical questions, which would make receiving medical advice and information more accessible with a fair level of reliability. However, in the context of medicine, AI hallucinations in text generation and the unreliability of plain LLMs makes the application of plain text generation in medical advice impractical due to the importance of having reliable and supported data. Therefore, RAG is a more practical approach in the application of new generative AI practices in the medical field. RAG involves two main steps from start to finish: context retrieval and answer generation. In this project, the Bidirectional Encoder Representations from Transformers (BERT) model retrieves the context from a large dataset, and the Generative Pre-trained Transformer 2 (GPT-2) model generates an answer using the retrieved context. 

2 Methodology 
The dataset utilized in this project is the PubMedQA dataset containing ~211k labeled samples. Each sample contains 4 important fields: the question (Y/N), context, long answer, and final decision (Y/N). Preprocessing for the dataset involved compiling the question + context as the question/input and compiling the long answer + final decision as the model label/ground truth. Each model uses their respective tokenizers in order to put the data into a suitable form for processing. The dataset is split in an 80/10/10 training, validation, and testing split. The RAG system takes a medical question as the input and returns a ‘yes’ or ‘no’ answer with reasoning as an output. As mentioned above, the input is first queried to the BERT model, which pulls the most relevant context from the dataset. This context is then passed to the GPT-2 model, which generates a final answer and reasoning using the context. This pipeline is depicted in Figure 1. The specific pre-trained BERT model used is the BertForQuestionAnswering, which is pre-trained to return the context from a dataset to answer a question. PyTorch serves as the core framework, providing tools for model training, inference, and backpropagation. Pre-trained BERT and GPT-2 models are accessed from HuggingFace's Transformers library, and Google Colab provides a cloud-based environment with GPU support, used to facilitate all fine-tuning, inference, and evaluation. Figure 1. Data pipeline including BERT and GPT2 models Both the pre-trained BertForQuestionAnswering and GPT2LMHeadModel are fine-tuned on the training dataset, which is roughly ~170k samples, for 3 epochs. Using Google Colab+ provided access to A100 GPUs which allow for faster fine-tuning time and execution. Overall, fine-tuning the BERT model took ~1 hour and fine-tuning the GPT-2 model took ~1 hour. The BLEU score is used in this task to evaluate the quality of the GPT-2 model's generated answers by comparing them to reference answers from the PubMedQA dataset. BLEU measures the overlap of n-grams (sequences of words) between the generated and reference texts, rewarding precise matches while penalizing both extra and missing words. However, because GPT-2 generates free-form text rather than exact matches, the BLEU score for this task is relatively low, highlighting the challenges of finding a suitable metric to grade the quality of generated text. 

3 Results 
BLEU Score (for GPT-2): Average BLEU Score: 0.1997 Example question and inference: Question: "Is magnesium deficiency associated with insulin resistance in obese children?" Generated Answer:  Is magnesium deficiency associated with insulin resistance in obese children? It is concluded that oral magnesium administration given at the time of diabetes induction may be able to restore thermal hyperalgesia, magnesium deficiency and hyperglycemia and in diabetic rats.Our results provide evidence that the presence of diabetes and the degree of metabolic control are essential in accounting for the lower levels of magnesium that exist in obese subjects. Reference Answer: Is magnesium deficiency associated with insulin resistance in obese children? Yes, the association between magnesium deficiency and IR is present during childhood. Serum magnesium deficiency in obese children may be secondary to decreased dietary magnesium intake. Magnesium supplementation or increased intake of magnesium-rich foods may be an important tool in the prevention of type 2 diabetes in obese children. 

4 Discussion 
While the BLEU score of 0.1997 suggests poor performance in the RAG, the BLEU score may not be the most suitable metric to grade the accuracy of this specific project. The low BLEU score highlights challenges in evaluating models designed to generate nuanced medical responses, where exact n-gram overlap is often less important than contextual accuracy and completeness. With differences in the system design, the BLEU score may improve while the actual performance of the RAG system may be the same. Examining the example question and inference provides more insight into both the strengths and shortcomings of the implemented RAG system. One issue I ran into was the 512 character limit of the BERT tokenizers– instead of passing in each sample’s given ‘context’ dictionary, I opted to use the ‘long_answer’ field, which explains the answer directly a little better and does some work for the GPT-2 model by providing a more “digestible” context. However, if the RAG model is aimed at providing an accurate answer while providing experimental evidence, implementing a more robust tokenizer to use each sample’s given ‘context’ field is ideal. In conclusion, the generated answer demonstrates some ability to synthesize information but lacks the concise phrasing of the reference answer. Another issue in this model is the inference time. The inference time for one sample was ~15 minutes due to the fact that the BERT model is prompted to find the most relevant context/sample in the dataset, meaning the BERT model must iterate through and grade each of the 200k+ samples in the dataset. One possible approach to reduce inference time could be referencing keywords in the question to shrink the scope of the dataset for each inference by sorting the dataset into specific subjects (e.g. “diabetes”, “magnesium”). Despite these limitations, the RAG approach shows promise for answering complex medical questions by combining retrieval and generation effectively.
